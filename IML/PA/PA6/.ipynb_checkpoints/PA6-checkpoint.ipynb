{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## Subtask 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('regression.h5', 'r')\n",
    "x_train = np.array(hf.get('x_train'))\n",
    "y_train = np.array(hf.get('y_train'))\n",
    "x_test = np.array(hf.get('x_test'))\n",
    "y_test = np.array(hf.get('y_test'))\n",
    "hf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedKernelizedLogisticRegression:\n",
    "    \"\"\"\n",
    "    Regularized Kernelized Logistic Regression.\n",
    "\n",
    "    Parameters:\n",
    "    - kernel_function: Callable\n",
    "        A kernel function that computes the kernel matrix for the input data.\n",
    "    - alpha: float, optional (default=1e-5)\n",
    "        Regularization parameter controlling the strength of regularization.\n",
    "    - max_iter: int, optional (default=1000)\n",
    "        Maximum number of iterations for the optimization algorithm.\n",
    "    - random_state: int, optional (default=42)\n",
    "        Seed for reproducibility of random initialization.\n",
    "\n",
    "    Attributes:\n",
    "    - kernel_function: Callable\n",
    "        The provided kernel function.\n",
    "    - alpha: float\n",
    "        Regularization parameter.\n",
    "    - max_iter: int\n",
    "        Maximum number of iterations for optimization.\n",
    "    - random_state: int\n",
    "        Seed for random initialization.\n",
    "    - w: numpy.ndarray\n",
    "        Optimized weights after fitting the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_function, alpha=1e-5, max_iter=1000, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the RegularizedKernelizedLogisticRegression instance.\n",
    "\n",
    "        Parameters:\n",
    "        - kernel_function: Callable\n",
    "            A kernel function that computes the kernel matrix for the input data.\n",
    "        - alpha: float, optional (default=1e-5)\n",
    "            Regularization parameter controlling the strength of regularization.\n",
    "        - max_iter: int, optional (default=1000)\n",
    "            Maximum number of iterations for the optimization algorithm.\n",
    "        - random_state: int, optional (default=42)\n",
    "            Seed for reproducibility of random initialization.\n",
    "        \"\"\"\n",
    "        self.kernel_function = kernel_function\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - x: numpy.ndarray\n",
    "            Training data.\n",
    "        - y: numpy.ndarray\n",
    "            Target values.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        n_samples, _ = x.shape\n",
    "\n",
    "        # Precompute kernel matrix\n",
    "        K = self.kernel_function(x)\n",
    "\n",
    "        # Initialize weights\n",
    "        np.random.seed(self.random_state)\n",
    "        self.w = np.random.rand(n_samples)\n",
    "\n",
    "        # Define logistic loss and its gradient\n",
    "        def logistic_loss(w):\n",
    "            logits = K @ w\n",
    "            logits = logits.clip(-500, 500)\n",
    "            predictions = 1 / (1 + np.exp(-logits))\n",
    "            epsilon = 1e-5\n",
    "            loss = -np.sum(y * np.log(predictions + epsilon) + (1 - y) * np.log(1 - predictions + epsilon)) / n_samples\n",
    "            regularization_term = self.alpha * w @ (K @ w)\n",
    "            return loss + regularization_term\n",
    "\n",
    "        def logistic_gradient(w):\n",
    "            logits = K @ w\n",
    "            logits = logits.clip(-500, 500)\n",
    "            predictions = 1 / (1 + np.exp(-logits))\n",
    "            error = predictions - y\n",
    "            gradient = (K.T @ error) / n_samples + 2 * self.alpha * (K @ w)\n",
    "            return gradient\n",
    "\n",
    "        # Minimize the logistic loss\n",
    "        result = minimize(logistic_loss, self.w, jac=logistic_gradient, method='L-BFGS-B', options={'maxiter': self.max_iter})\n",
    "\n",
    "        # Store the optimized weights\n",
    "        self.w = result.x\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict class labels for new data.\n",
    "\n",
    "        Parameters:\n",
    "        - x: numpy.ndarray\n",
    "            New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray\n",
    "            Predicted class labels (binary).\n",
    "        \"\"\"\n",
    "        # Compute kernel matrix for new data\n",
    "        K_new = self.kernel_function(x)\n",
    "\n",
    "        # Compute logits and predict probabilities\n",
    "        logits = K_new @ self.w\n",
    "        logits = logits.clip(-500, 500)\n",
    "        probabilities = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "        # Convert probabilities to binary predictions\n",
    "        predictions = (probabilities >= 0.5).astype(int)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(x):\n",
    "    \"\"\"\n",
    "    Calculate the Radial Basis Function (RBF) kernel matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input data, where each row represents a data point.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: RBF kernel matrix.\n",
    "    \"\"\"\n",
    "    # Calculate pairwise squared Euclidean distances\n",
    "    dists = np.square(np.linalg.norm(x[:, np.newaxis] - x, axis=2))\n",
    "    \n",
    "    # Compute the RBF kernel matrix\n",
    "    K = np.exp(-0.5 * dists)\n",
    "    \n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on your data and transform it\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.fit_transform(x_test)\n",
    "\n",
    "# for the first subtask we can only use the first 2 features\n",
    "x_train_scaled_geo = x_train_scaled[:, :2]\n",
    "x_test_scaled_geo = x_test_scaled[:, :2]\n",
    "\n",
    "# splitting the two target variables separately\n",
    "y_train_chimney = y_train[:, 0]\n",
    "y_test_chimney = y_test[:, 0]\n",
    "y_test_children = y_test[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the alphas to consider\n",
    "alphas = [10, 5, 1, 0.1, 1e-3, 1e-5]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# chossing the best alpha wrt. the validation set accuracy (hyperparameter tuning)\n",
    "for alpha in alphas:\n",
    "    \n",
    "    model = RegularizedKernelizedLogisticRegression(rbf, alpha)\n",
    "    model.fit(x_train_scaled_geo, y_train_chimney)\n",
    "    y_test_pred = model.predict(x_test_scaled_geo)\n",
    "    accuracy_test = accuracy_score(y_test_chimney, y_test_pred)\n",
    "    # if the new model has a better accuracy than previous data, update it\n",
    "    if accuracy_test > best_accuracy:\n",
    "        best_accuracy = accuracy_test\n",
    "        best_model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy on the training data: 64.9%\n",
      "The classification accuracy on the validation data: 58.099999999999994%\n"
     ]
    }
   ],
   "source": [
    "# reporting the classification accuracy on the training and validation data\n",
    "y_train_pred = best_model.predict(x_train_scaled_geo)\n",
    "training_accuracy = accuracy_score(y_train_chimney, y_train_pred)\n",
    "\n",
    "print(f'The classification accuracy on the training data: {training_accuracy*100}%')\n",
    "print(f'The classification accuracy on the validation data: {best_accuracy*100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_kernel(x):\n",
    "    \"\"\"\n",
    "    Computes a custom kernel function as a combination of three RBF kernels.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Input data matrix.\n",
    "\n",
    "    Returns:\n",
    "    - The result of the custom kernel computation.\n",
    "    \"\"\"\n",
    "    return rbf(x)**3 + rbf(x)**2 + rbf(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we knew that a polynomial with positive coefficients of a kernel is a kernel, I tried out some polynomial variations of the `rbf` kernel, and with the `rbf^3 + rbf^2 + rbf` kernel and with using all the available features I was able to achieve better accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_improved_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# chossing the best alpha wrt. the validation set accuracy (hyperparameter tuning)\n",
    "# we use the same alpha pool as before\n",
    "for alpha in alphas:\n",
    "    \n",
    "    improved_model = RegularizedKernelizedLogisticRegression(custom_kernel, alpha)\n",
    "    improved_model.fit(x_train_scaled, y_train_chimney)\n",
    "    y_test_pred = improved_model.predict(x_test_scaled)\n",
    "    accuracy_test_improved = accuracy_score(y_test_chimney, y_test_pred)\n",
    "    # if the new model has a better accuracy than previous data, update it\n",
    "    if accuracy_test_improved > best_accuracy:\n",
    "        best_accuracy = accuracy_test_improved\n",
    "        best_improved_model = improved_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy on the training data: 79.10000000000001%\n",
      "The classification accuracy on the validation data: 64.3%\n",
      "The selected alpha hyperparameter: 1\n"
     ]
    }
   ],
   "source": [
    "# reporting the classification accuracy on the training and validation data and the chosenalpha hyperparameter\n",
    "y_train_pred = best_improved_model.predict(x_train_scaled)\n",
    "training_accuracy = accuracy_score(y_train_chimney, y_train_pred)\n",
    "\n",
    "print(f'The classification accuracy on the training data: {training_accuracy*100}%')\n",
    "print(f'The classification accuracy on the validation data: {best_accuracy*100}%')\n",
    "print(f'The selected alpha hyperparameter: {best_improved_model.alpha}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 3\n",
    "Similarly to the original cost function, our cost function will be:\n",
    "\n",
    "`c(y, y^) = y⋅(1−y^)−(1−y)⋅(1−y^)-10⋅(1−y)⋅(y^) = y⋅(1−y^)−(1−y)⋅(9⋅(y^) + 1)`\n",
    "\n",
    "where:\n",
    "- y is the true label,\n",
    "- y^ is the predicted probability\n",
    "\n",
    "as in this case for the inputs given in the description the ouput is always the required value, because in this case `y` denotes if Santa really visits the given house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_support(a, y):\n",
    "    \"\"\"\n",
    "    Calculates the custom logistic regression cost function with support for top predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - a: Predicted probabilities.\n",
    "    - y: True labels.\n",
    "\n",
    "    Returns:\n",
    "    - The custom logistic regression cost.\n",
    "    \"\"\"\n",
    "    n_samples = len(y)\n",
    "    cost = np.sum(y * (1 - a) + (1 - y) * (9 * a + 1)) / n_samples\n",
    "    \n",
    "    return cost\n",
    "\n",
    "class RegularizedKernelizedLogisticRegressionSupport:\n",
    "    def __init__(self, kernel_function, alpha=1e-5, max_iter=1000, random_state=42, limit=50):\n",
    "        \"\"\"\n",
    "        Regularized Kernelized Logistic Regression model with support for top predictions.\n",
    "\n",
    "        Parameters:\n",
    "        - kernel_function: Callable\n",
    "            A kernel function that computes the kernel matrix for the input data.\n",
    "        - alpha: float, optional (default=1e-5)\n",
    "            Regularization parameter controlling the strength of regularization.\n",
    "        - max_iter: int, optional (default=1000)\n",
    "            Maximum number of iterations for the optimization algorithm.\n",
    "        - random_state: int, optional (default=42)\n",
    "            Seed for reproducibility of random initialization.\n",
    "        - limit: int, optional (default is 50)\n",
    "            Number of top predictions to consider\n",
    "        \n",
    "\n",
    "        Attributes:\n",
    "        - kernel_function: Callable\n",
    "            The provided kernel function.\n",
    "        - alpha: float\n",
    "            Regularization parameter.\n",
    "        - max_iter: int\n",
    "            Maximum number of iterations for optimization.\n",
    "        - random_state: int\n",
    "            Seed for random initialization.\n",
    "        - w: numpy.ndarray\n",
    "            Optimized weights after fitting the model.\n",
    "        \"\"\"\n",
    "        self.kernel_function = kernel_function\n",
    "        self.alpha = alpha  # regularization parameter\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.limit = limit\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - x: numpy.ndarray\n",
    "            Training data.\n",
    "        - y: numpy.ndarray\n",
    "            Target values.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        n_samples, _ = x.shape\n",
    "\n",
    "        # Precompute kernel matrix\n",
    "        K = self.kernel_function(x)\n",
    "\n",
    "        # Initialize weights\n",
    "        np.random.seed(self.random_state)\n",
    "        self.w = np.random.rand(n_samples)\n",
    "\n",
    "        # Define logistic loss and its gradient\n",
    "        def logistic_loss(w):\n",
    "            logits = K @ w\n",
    "            logits = logits.clip(-500, 500)\n",
    "            probabilities = 1 / (1 + np.exp(-logits))\n",
    "            # Find the indices of the 50 largest values\n",
    "            top_indices = np.argsort(probabilities)[-self.limit:]\n",
    "\n",
    "            # Create a new array of zeros\n",
    "            predictions = np.zeros_like(probabilities)\n",
    "\n",
    "            # Set the elements at the identified indices to 1\n",
    "            predictions[top_indices] = 1\n",
    "\n",
    "            loss = cost_support(y, predictions)\n",
    "            regularization_term = self.alpha * w @ (K @ w)\n",
    "            return loss + regularization_term\n",
    "\n",
    "        def logistic_gradient(w):\n",
    "            logits = K @ w\n",
    "            logits = logits.clip(-500, 500)\n",
    "            predictions = 1 / (1 + np.exp(-logits))\n",
    "            error = predictions - y\n",
    "            gradient = (K.T @ error) / n_samples + 2 * self.alpha * (K @ w)\n",
    "            return gradient\n",
    "\n",
    "        # Minimize the logistic loss\n",
    "        result = minimize(logistic_loss, self.w, jac=logistic_gradient, method='L-BFGS-B', options={'maxiter': self.max_iter})\n",
    "\n",
    "        # Store the optimized weights\n",
    "        self.w = result.x\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict class labels for new data.\n",
    "\n",
    "        Parameters:\n",
    "        - x: numpy.ndarray\n",
    "            New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray\n",
    "            Predicted class labels (binary).\n",
    "        \"\"\"\n",
    "        # Compute kernel matrix for new data\n",
    "        K_new = self.kernel_function(x)\n",
    "\n",
    "        # Compute logits and predict probabilities\n",
    "        logits = K_new @ self.w\n",
    "        logits = logits.clip(-500, 500)\n",
    "        probabilities = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "        # Find the indices of the 50 largest values\n",
    "        top_indices = np.argsort(probabilities)[-self.limit:]\n",
    "\n",
    "        # Create a new array of zeros\n",
    "        predictions = np.zeros_like(probabilities)\n",
    "\n",
    "        # Set the elements at the identified indices to 1\n",
    "        predictions[top_indices] = 1\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost achieved: 1.25\n"
     ]
    }
   ],
   "source": [
    "# the alphas to consider during hyperparameter tuning\n",
    "alphas = [5000, 1000, 100, 10, 5, 1, 0.1, 1e-3, 1e-5]\n",
    "\n",
    "best_model = None\n",
    "best_cost = np.inf\n",
    "\n",
    "# chossing the best alpha wrt. the cost of the validation set (hyperparameter tuning)\n",
    "for alpha in alphas:\n",
    "    \n",
    "    model = RegularizedKernelizedLogisticRegressionSupport(custom_kernel, alpha)\n",
    "    model.fit(x_train_scaled, y_train_chimney)\n",
    "    y_test_pred = model.predict(x_test_scaled)\n",
    "    cost = cost_support(y_test_pred, y_test_chimney)\n",
    "    # if the new model has a better accuracy than previous data, update it\n",
    "    if cost < best_cost:\n",
    "        best_cost = cost\n",
    "        best_model = model\n",
    "\n",
    "print(f'The cost achieved: {best_cost}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost achieved in a random way: 1.2706000000000004\n"
     ]
    }
   ],
   "source": [
    "# calculating the cost on random chossing 50 houses for Santa to visit\n",
    "num_random_selections = 50\n",
    "limit = 50\n",
    "num_of_samples = len(y_test_chimney)\n",
    "\n",
    "cost_random = []\n",
    "\n",
    "for i in range(num_random_selections):\n",
    "    np.random.seed(i)\n",
    "\n",
    "    # Create an array of length 50 with all zeros\n",
    "    y_test_pred_random = np.zeros(num_of_samples)\n",
    "\n",
    "    # Choose 10 random indices to set to 1\n",
    "    random_indices = np.random.choice(num_of_samples, limit, replace=False)\n",
    "\n",
    "    # Set the chosen indices to 1\n",
    "    y_test_pred_random[random_indices] = 1\n",
    "    \n",
    "    cost = cost_support(y_test_pred_random, y_test_chimney)\n",
    "    cost_random.append(cost)\n",
    "\n",
    "cost_random = sum(cost_random) / len(cost_random)\n",
    "# reporting the average cost of 50 random samples\n",
    "print(f'The cost achieved in a random way: {cost_random}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning that my model has only a slightly better performance than picking the houses randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Santa visits 30 houses with children\n"
     ]
    }
   ],
   "source": [
    "# reporting the houses with children Santa visited\n",
    "visited_houses_with_children = np.sum(np.logical_and(y_test_children, y_test_pred).astype(int))\n",
    "print(f'Santa visits {visited_houses_with_children} houses with children')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
